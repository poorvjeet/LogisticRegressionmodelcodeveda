{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YKztzFZ6kMD"
      },
      "outputs": [],
      "source": [
        "# Task 3 — NLP (Sentiment Classification) — Full Colab Notebook\n",
        "\n",
        "## 1. Setup & Installation\n",
        "!pip install pandas numpy matplotlib scikit-learn seaborn transformers datasets --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "print(\"Setup complete.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "## 2. Load Dataset - using 20 Newsgroups as sample sentiment dataset\n",
        "# -------------------------------------------------------------\n",
        "print(\"Loading a sample sentiment dataset (20 Newsgroups subset).\")\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# We'll use 3 categories for demonstration\n",
        "categories = ['rec.sport.baseball', 'sci.space', 'talk.politics.guns']\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=categories,\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "newsgroups_test = fetch_20newsgroups(\n",
        "    subset='test',\n",
        "    categories=categories,\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "# Convert integer targets into category names\n",
        "train_labels = [newsgroups_train.target_names[i] for i in newsgroups_train.target]\n",
        "test_labels = [newsgroups_test.target_names[i] for i in newsgroups_test.target]\n",
        "\n",
        "# Build DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'text': newsgroups_train.data + newsgroups_test.data,\n",
        "    'label': train_labels + test_labels\n",
        "})\n",
        "\n",
        "print(\"Sample sentiment dataset loaded. Shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "## 3. Preprocessing — clean labels, group top categories\n",
        "# -------------------------------------------------------------\n",
        "df = df[['text', 'label']].dropna()\n",
        "df['label'] = df['label'].astype(str).str.strip().str.lower()\n",
        "\n",
        "# Ensure we don’t group beyond available labels\n",
        "unique_labels = df['label'].nunique()\n",
        "top_n = min(5, unique_labels)\n",
        "\n",
        "# Group top-n labels + 'other'\n",
        "top_labels = df['label'].value_counts().nlargest(top_n).index.tolist()\n",
        "df['label_grouped'] = df['label'].apply(lambda x: x if x in top_labels else 'other')\n",
        "\n",
        "print(\"Label distribution:\")\n",
        "print(df['label_grouped'].value_counts())\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "## 4. Train-Test Split & TF-IDF\n",
        "# -------------------------------------------------------------\n",
        "X = df['text'].astype(str)\n",
        "y = df['label_grouped']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "vec = TfidfVectorizer(stop_words='english', max_features=10000)\n",
        "X_train_t = vec.fit_transform(X_train)\n",
        "X_test_t = vec.transform(X_test)\n",
        "\n",
        "print(\"Vectorization complete. Train shape:\", X_train_t.shape)\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "## 5. Classical Models\n",
        "# -------------------------------------------------------------\n",
        "# MultinomialNB\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_t, y_train)\n",
        "pred_mnb = mnb.predict(X_test_t)\n",
        "print(\"MultinomialNB Accuracy:\", accuracy_score(y_test, pred_mnb))\n",
        "print(classification_report(y_test, pred_mnb))\n",
        "\n",
        "# Logistic Regression (balanced)\n",
        "lr = LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear')\n",
        "lr.fit(X_train_t, y_train)\n",
        "pred_lr = lr.predict(X_test_t)\n",
        "print(\"LogReg Accuracy:\", accuracy_score(y_test, pred_lr))\n",
        "print(classification_report(y_test, pred_lr))\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "## 6. Confusion Matrix for Logistic Regression\n",
        "# -------------------------------------------------------------\n",
        "cm = confusion_matrix(y_test, pred_lr, labels=lr.classes_)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=lr.classes_, yticklabels=lr.classes_)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "## 7. Optional: Fine-tune DistilBERT (requires GPU)\n",
        "# -------------------------------------------------------------\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available. Proceeding with DistilBERT fine-tuning.\")\n",
        "\n",
        "    from datasets import Dataset\n",
        "    from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "    # Sample for faster demo\n",
        "    sample_size = min(5000, len(df))\n",
        "    df_small = df.sample(sample_size, random_state=42)\n",
        "\n",
        "    dataset = Dataset.from_pandas(df_small[['text','label_grouped']])\n",
        "\n",
        "    # Encode labels\n",
        "    labels = list(df_small['label_grouped'].unique())\n",
        "    label2id = {l:i for i,l in enumerate(labels)}\n",
        "    id2label = {i:l for l,i in label2id.items()}\n",
        "    dataset = dataset.map(lambda e: {'label_id': [label2id[x] for x in e['label_grouped']]}, batched=True)\n",
        "\n",
        "    # Tokenization\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=256)\n",
        "    dataset = dataset.map(tokenize, batched=True)\n",
        "\n",
        "    # Split\n",
        "    dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    train_ds, test_ds = dataset['train'], dataset['test']\n",
        "    train_ds.set_format(type='torch', columns=['input_ids','attention_mask','label_id'])\n",
        "    test_ds.set_format(type='torch', columns=['input_ids','attention_mask','label_id'])\n",
        "\n",
        "    # Model\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        'distilbert-base-uncased',\n",
        "        num_labels=len(labels),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    # Training args\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=2,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=test_ds,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "else:\n",
        "    print(\"GPU not available. Skipping DistilBERT fine-tuning.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "## 8. Save Models & Vectorizer\n",
        "# -------------------------------------------------------------\n",
        "import joblib\n",
        "\n",
        "joblib.dump(vec, \"tfidf_vectorizer.pkl\")\n",
        "joblib.dump(mnb, \"model_mnb.pkl\")\n",
        "joblib.dump(lr, \"model_logreg.pkl\")\n",
        "\n",
        "print(\"Models saved locally.\")\n",
        "print(\"Process completed successfully!\")\n"
      ]
    }
  ]
}